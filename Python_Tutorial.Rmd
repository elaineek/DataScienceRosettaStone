---
title: "Data Science Using Python, SAS, & R"
author: "Elaine Kearney"
subtitle: A Rosetta Stone for Analytical Languages
output:
  html_document:
    df_print: paged
    highlight: kate
    toc: yes
    toc_float: yes
  word_document:
    highlight: kate
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, engine = 'python', engine.path="/Users/elaineek/anaconda/bin/python", highlight = TRUE)
```

# Python Tutorial

Welcome to the Python tutorial version of *Data Science Using Python, SAS, & R: A Rosetta Stone for Analytical Languages*.  This tutorial includes examples of common data science tasks, organized in the same way across 3 data science languages.  Before beginning this tutorial, please check to make sure you have Python 3.5.2 installed (this is not required, but this was the release used to generate the following examples).  Also, the following Python packages are used throughout this tutorial.  You may not need all of the following packages to fit your specific needs, but they are listed below, and also in Appendix Section 2 with more detail, for your information:

[pandas (0.20.2)](#PANDAS) | [NumPy (1.12.1)](#NUMPY) | [Matplotlib.PyPlot](#PYPLOT) | [seaborn (0.7.1)](#SEABORN) | [re (2.2.1)](#re) | [decimal (1.70)](#DECIMAL) | [sklearn (0.18.2)](#SKLEARN) | [statsmodels.api](#STATSMODELS) | [xgboost (0.6)](#XGBOOST) | [pyclustering](#PYCLUSTERING) | [PyFlux (0.4.15)](#PYFLUX) | [fbprophet](#FBPROPHET)

To install Python packages you will often need to run the following code from a terminal/command line on your computer, and then later in a Python environment you will import the package, which is demonstrated in this tutorial:

```{python, eval = FALSE}
pip install package_name
# or #
conda install package_name
```

Note: In Python, comments are indicated in code with a "#" character, and arrays and matrices are zero-indexed.

Now let's get started!  First, you need to import several very important Python packages for data manipulation and scientific computing.  The [pandas](#PANDAS) package is useful for data manipulation and the [NumPy](#NUMPY) package is useful for scientific computing. 
```{python, eval = FALSE}
import pandas as pd
import numpy as np
```

*** 

# 1 Reading in Data and Basic Statistical Functions

## 1.1 Read in the data.

The following demonstrate importing data into Python given 3 different file formats.  The pandas package is able to read all 3 formats, as well as many others, using [Python IO tools](http://pandas.pydata.org/pandas-docs/version/0.20/io.html). 

### a) Read the data in as a .csv file.

```{python, eval = FALSE}
student = pd.read_csv('/Users/class.csv')
```

### b) Read the data in as a .xls file.

```{python, eval = FALSE}
# Notice you must specify the file location, as well as the name of the sheet 
# of the .xls file you want to import
student_xls = pd.read_excel(open('/Users/class.xls', 'rb'), 
                            sheetname='class')
```

### c) Read the data in as a .json file.

```{python, eval = FALSE}
student_json = pd.read_json('/Users/class.json')
```

## 1.2 Find the dimensions of the data set.

The dimensions of a [DataFrame](#DataFrame) in Python are known as an attribute of the object.  Therefore, you can state the data name followed by [.shape](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.shape.html) to return the dimensions of the data, with the first integer indicating the number of rows and the second indicating the number of columns.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.shape)
```

## 1.3 Find basic information about the data set.

Information about a [DataFrame](#DataFrame) is available by calling the [info()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html) function on the data.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.info())
```

## 1.4 Look at the first 5 (last 5) observations.

The first 5 observations of a [DataFrame](#DataFrame) are available by calling the [head()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html) function on the data.  By default, head() returns 5 observations.  To return the first *n* observations, pass the integer *n* into the function.  The [tail()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html) function is analogous and returns the last observations.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.head())
```

## 1.5 Calculate means of numeric variables.

The means of numeric variables of a [DataFrame](#DataFrame) are available by calling the [mean()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mean.html) function on the data.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.mean())
```

## 1.6 Compute summary statistics of the data set.

Summary statistics of a [DataFrame](#DataFrame) are available by calling the [describe()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html) function on the data.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.describe())
```

## 1.7 Descriptive statistics functions applied to variables of the data set.

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice the subsetting of student with [] and the name of the variable in 
# quotes ("")
print(student["Weight"].std())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].sum())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].count())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].max())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].min())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].median())
```

## 1.8 Produce a one-way table to describe the frequency of a variable.

### a) Produce a one-way table of a discrete variable.
```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# columns = "count" indicates to make the descriptive portion of the table 
# the counts of each level of the index variable
print(pd.crosstab(index=student["Age"], columns="count"))
```

### b) Produce a one-way table of a categorical variable.
```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(pd.crosstab(index=student["Sex"], columns="count"))
```
[pd.crosstab()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html)

## 1.9 Produce a two-way table to describe the frequency of two categorical or discrete variables.

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice the specification of a variable for the columns argument, instead 
# of "count"
print(pd.crosstab(index=student["Age"], columns=student["Sex"]))
```
[pd.crosstab()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html)

## 1.10 Select a subset of the data that meets a certain criterion.

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
females = student.query('Sex == "F"')
print(females.head())
```
[query()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html)

## 1.11 Determine the correlation between two continuous variables.

```{python, echo = 3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# axis = 1 option indicates to concatenate column-wise
height_weight = pd.concat([student["Height"], student["Weight"]], axis = 1)
print(height_weight.corr(method = "pearson"))
```
[pd.concat()](http://pandas.pydata.org/pandas-docs/version/0.20/generated/pandas.concat.html) | [corr()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html)

***

\newpage

# 2 Basic Graphing and Plotting Functions

The Matplotlib [PyPlot](#PYPLOT) package is a standard Python package to use for plotting.  For more information on other Python plotting packages, please see the Appendix Section 2.

```{python}
import matplotlib.pyplot as plt
```

## 2.1 Visualize a single continuous variable by producing a histogram.

```{python, eval=FALSE}
# Notice the labeling of the axes
plt.hist(student["Weight"], bins=[40,60,80,100,120,140,160])
plt.xlabel('Weight')
plt.ylabel('Frequency')
plt.show()
```

Output:
![output](histogram2.png)

## 2.2 Visualize a single continuous variable by producing a boxplot.

```{python, eval=FALSE}
# showmeans=True tells Python to plot the mean of the variable on the boxplot 
plt.boxplot(student["Weight"], showmeans=True)

# prevents Python from printing a "1" at the bottom of the boxplot
plt.xticks([])

plt.ylabel('Weight')
plt.show()
```

Output:
![output](boxplot.png)

## 2.3 Visualize two continuous variables by producing a scatterplot.

```{python, eval=FALSE}
# Notice here you specify the x variable, followed by the y variable 
plt.scatter(student["Height"], student["Weight"])
plt.xlabel("Height")
plt.ylabel("Weight")
plt.show()
```

Output:
![output](scatter.png)

## 2.4 Visualize a relationship between two continuous variables by producing a scatterplot and a plotted line of best fit.

```{python, eval=FALSE}
x = student["Height"]
y = student["Weight"]

# np.polyfit() models Weight as a function of Height and returns the 
# parameters
m, b = np.polyfit(x, y, 1)
plt.scatter(x, y)

# plt.text() prints the equation of the line of best fit, with the first two 
# arguments specifying the x and y locations of the text, respectively 
# "%f" indicates to print a floating point number, that is specified following
# the string and a "%" character
plt.text(51, 140, "Line: y = %f x + %f"% (m,b))
plt.plot(x, m*x + b)
plt.xlabel("Height")
plt.ylabel("Weight")
plt.show()
```

Output:
![output](lineofbestfit.png)

[np.polyfit()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html)

## 2.5 Visualize a categorical variable by producing a bar chart.

```{python, eval=FALSE}
# Get the counts of Sex 
counts = pd.crosstab(index=student["Sex"], columns="count")

# len() returns the number of categories of Sex (2)
# np.arange() creates a vector of the specified length
num = np.arange(len(counts))

# alpha = 0.5 changes the transparency of the bars
plt.bar(num, counts["count"], align='center', alpha=0.5)

# Set the xticks to be the indices of counts
plt.xticks(num, counts.index)
plt.xlabel("Sex")
plt.ylabel("Frequency")
plt.show()
```

Output:
![output](barchart.png)

[np.arange()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html)

## 2.6 Visualize a continuous variable, grouped by a categorical variable, by producing side-by-side boxplots.

### a) Simple side-by-side boxplot without color.
```{python, eval=FALSE}
# Subset data set to return only female weights, and then only male weights 
Weight_F = np.array(student.query('Sex == "F"')["Weight"])
Weight_M = np.array(student.query('Sex == "M"')["Weight"])
Weights = [Weight_F, Weight_M]

# PyPlot automatically plots the two weights side-by-side since Weights 
# is a 2D array
plt.boxplot(Weights, showmeans=True, labels=('F', 'M'))
plt.xlabel('Sex')
plt.ylabel('Weight')
plt.show()
```

Output:
![output](sboxplot.png)

[np.array()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)

### b) More advanced side-by-side boxplot with color.

```{python, eval=FALSE}
import seaborn as sns
sns.boxplot(x="Sex", y="Weight", hue="Sex", data = student, showmeans=True)
sns.plt.show()
```

Output:
![output](snsboxplot.png)

[seaborn](#SEABORN)

***

# 3 Basic Data Wrangling and Manipulation

## 3.1 Create a new variable in a data set as a function of existing variables in the data set.

```{python, echo=3:6}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice here how you can create the BMI column in the data set 
# just by naming it 
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
print(student.head())
```

## 3.2 Create a new variable in a data set using if/else logic of existing variables in the data set.

```{python, echo=5:8}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
# Notice the use of the np.where() function for a single condition 
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", 
                                "Healthy")
print(student.head())
```

[np.where()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html)

## 3.3 Create new variables in a data set using mathematical functions applied to existing variables in the data set.

Using the [np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html),  [np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html),  [np.sqrt()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html), [np.where()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html), and [np.abs()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.absolute.html) functions.
```{python, echo=6:14}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student["LogWeight"] = np.log(student["Weight"])
student["ExpAge"] = np.exp(student["Age"])
student["SqrtHeight"] = np.sqrt(student["Height"])
student["BMI Neg"] = np.where(student["BMI"] < 19.0, -student["BMI"], 
                              student["BMI"])
student["BMI Pos"] = np.abs(student["BMI Neg"])

# Create a Boolean variable
student["BMI Check"] = (student["BMI Pos"] == student["BMI"])
print(student.head())
```

## 3.4 Drop variables from a data set.

```{python, echo=12:15}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student["LogWeight"] = np.log(student["Weight"])
student["ExpAge"] = np.exp(student["Age"])
student["SqrtHeight"] = np.sqrt(student["Height"])
student["BMI Neg"] = np.where(student["BMI"] < 19.0, -student["BMI"], student["BMI"])
student["BMI Pos"] = np.abs(student["BMI Neg"])
student["BMI Check"] = (student["BMI Pos"] == student["BMI"])
# axis = 1 indicates to drop columns instead of rows
student = student.drop(["LogWeight", "ExpAge", "SqrtHeight", "BMI Neg", 
                        "BMI Pos", "BMI Check"], axis = 1)
print(student.head())
```
[drop()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)

## 3.5 Sort a data set by a variable.

### a) Sort data set by a continuous variable.

```{python, echo=6:9}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
# Notice kind="mergesort" which indicates to use a stable sorting 
# algorithm 
student = student.sort_values(by="Age", kind="mergesort")
print(student.head())
```

### b) Sort data set by a categorical variable.

```{python, echo=7:9}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind="mergesort")
student = student.sort_values(by="Sex", kind="mergesort")
# Notice that the data is now sorted first by Sex and then within Sex by Age 
print(student.head())
```
[sort_values()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html)

## 3.6 Compute descriptive statistics of continuous variables, grouped by a categorical variable.

```{python, echo=8}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
print(student.groupby(by="Sex").mean())
```
[groupby()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)

## 3.7 Add a new row to the bottom of a data set.

```{python, echo=8:9}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
# Look at the tail of the data currently
print(student.tail())
```

```{python, echo=8:16}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
student = student.append({'Name':'Jane', 'Sex':'F', 'Age':14, 'Height':56.3, 
                          'Weight':77.0, 'BMI':17.077695, 
                          'BMI Class': 'Underweight'}, 
                         ignore_index=True)

# Notice the change in the indices because of the ignore_index=True option 
# which allows for a Series, or one-dimensional DataFrame, to be appended 
# to an existing DataFrame 

print(student.tail())
```
[append()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html)

## 3.8 Create a user-defined function and apply it to a variable in the data set to create a new variable in the data set.

```{python, echo=12:16}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
student = student.append({'Name':'Jane', 'Sex':'F', 'Age':14, 'Height':56.3, 
                          'Weight':77.0, 'BMI':17.077695, 
                          'BMI Class': 'Underweight'}, 
                         ignore_index=True)
def toKG(lb):
    return (0.45359237 * lb)

student["Weight KG"] = student["Weight"].apply(toKG)
print(student.head())
```
[apply()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) | [user-defined functions](https://www.tutorialspoint.com/python/python_functions.htm)

***

\newpage

# 4 More Advanced Data Wrangling

## 4.1 Drop observations with missing information.

```{python, echo=2:8}
import pandas as pd
# Notice the use of the fish data set because it has some missing 
# observations 
fish = pd.read_csv('/Users/fish.csv')

# First sort by Weight, requesting those with NA for Weight first 
fish = fish.sort_values(by='Weight', kind='mergesort', na_position='first')
print(fish.head())
```

--

```{python, echo=4:5}
import pandas as pd
fish = pd.read_csv('/Users/fish.csv')
fish = fish.sort_values(by='Weight', kind='mergesort', na_position='first')
new_fish = fish.dropna()
print(new_fish.head())
```
[dropna()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html)

## 4.2 Merge two data sets together on a common variable.

### a) First, select specific columns of a data set to create two smaller data sets.

```{python, echo=2:7}
import pandas as pd
# Notice the use of the student data set again, however we want to reload it
# without the changes we've made previously 
student = pd.read_csv('/Users/class.csv')
student1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
print(student1.head())
```

--

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
student2 = pd.concat([student["Name"], student["Height"], student["Weight"]], 
                    axis = 1)
print(student2.head())
```

### b) Second, we want to merge the two smaller data sets on the common variable.

```{python, echo=7:8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
student1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
student2 = pd.concat([student["Name"], student["Height"], student["Weight"]], 
                    axis = 1)
new = pd.merge(student1, student2, on="Name")
print(new.head())
```
[pd.merge()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html)

### c) Finally, we want to check to see if the merged data set is the same as the original data set.

```{python, echo=8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
student1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
student2 = pd.concat([student["Name"], student["Height"], student["Weight"]], 
                    axis = 1)
new = pd.merge(student1, student2, on="Name")
print(student.equals(new))
```
[equals()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.equals.html)

## 4.3 Merge two data sets together by index number only.

### a) First, select specific columns of a data set to create two smaller data sets.

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                        axis = 1)
print(newstudent1.head())
```

--

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent2 = pd.concat([student["Height"], student["Weight"]], axis = 1)
print(newstudent2.head())
```

### b) Second, we want to join the two smaller data sets.

```{python, echo=7:8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
newstudent2 = pd.concat([student["Height"], student["Weight"]], 
                    axis = 1)
new2 = newstudent1.join(newstudent2)
print(new2.head())
```
[join()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html)

### c) Finally, we want to check to see if the joined data set is the same as the original data set.

```{python, echo=8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
newstudent2 = pd.concat([student["Height"], student["Weight"]], 
                    axis = 1)
new2 = newstudent1.join(newstudent2)
print(student.equals(new2))
```

## 4.4 Create a pivot table to summarize information about a data set.

```{python, echo=3:18}
import pandas as pd
import numpy as np
# Notice we are using a new data set that needs to be read into the 
# environment 
price = pd.read_csv('/Users/price.csv')

# The following code is used to remove the "," and "$" characters from 
# the ACTUAL colum so that the values can be summed 
from re import sub
from decimal import Decimal
def trim_money(money):
    return(float(Decimal(sub(r'[^\d.]', '', money))))

price["REVENUE"] = price["ACTUAL"].apply(trim_money)
table = pd.pivot_table(price, index=["COUNTRY", "STATE", "PRODTYPE", 
                                     "PRODUCT"], values="REVENUE", 
                       aggfunc=np.sum)
print(table.head())
```
Note: [pd.pivot_table()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html) is similar to the [pd.pivot()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html) function.

[re](#re) | [Decimal](#DECIMAL) 


## 4.5 Return all unique values from a text variable.
```{python, echo = 4}
import pandas as pd
import numpy as np
price = pd.read_csv('/Users/price.csv')
print(np.unique(price["STATE"]))
```
[np.unique()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html)

***

\newpage

The following sections focus on the Python [sklearn](#SKLEARN) package.  Also, in the following sections, several data set will be used more than once for prediction and modeling.  Often, they will be re-read into the environment so we are always going back to the original, raw data.

# 5 Preparation & Basic Regression

## 5.1 Pre-process a data set using principal component analysis.

```{python, echo = 3:15}
import pandas as pd
import numpy as np
# Notice we are using a new data set that needs to be read into the 
# environment 
iris = pd.read_csv('/Users/iris.csv')
features = iris.drop(["Target"], axis = 1)

from sklearn import preprocessing
features_scaled = preprocessing.scale(features.as_matrix())

from sklearn.decomposition import PCA

pca = PCA(n_components = 4)
pca = pca.fit(features_scaled)
print(np.transpose(pca.components_))
```
[preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html) | [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) | [np.transpose()](https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.transpose.html)

## 5.2 Split data into training and testing data and export as a .csv file.

```{python, eval = FALSE}
from sklearn.model_selection import train_test_split

target = iris["Target"]

# The following code splits the iris data set into 70% train and 30% test
X_train, X_test, Y_train, Y_test = train_test_split(features, target, 
                                                    test_size = 0.3, 
                                                    random_state = 29)
train_x = pd.DataFrame(X_train)
train_y = pd.DataFrame(Y_train)
test_x = pd.DataFrame(X_test)
test_y = pd.DataFrame(Y_test)

train = pd.concat([train_x, train_y], axis = 1)
test = pd.concat([test_x, test_y], axis = 1)

train.to_csv('/Users/iris_train_Python.csv', index = False)
test.to_csv('/Users/iris_test_Python.csv', index = False)
```
[train_test_split()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) | [to_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html)

## 5.3 Fit a logistic regression model.

```{python, echo=5:20}
import pandas as pd
import numpy as np
from pandas import to_datetime
from pandas import tseries
# Notice we are using a new data set that needs to be read into the 
# environment 
tips = pd.read_csv('/Users/tips.csv')

# The following code is used to determine if the individual left more 
# than a 15% tip 
tips["fifteen"] = 0.15 * tips["total_bill"]
tips["greater15"] = np.where(tips["tip"] > tips["fifteen"], 1, 0)

import statsmodels.api as sm

# Notice the syntax of greater15 as a function of total_bill 
logreg = sm.formula.glm("greater15 ~ total_bill", 
                       family=sm.families.Binomial(), 
                       data=tips).fit()
print(logreg.summary())
```
A logistic regression model can be implemented using [sklearn](#SKLEARN), however [statsmodels.api](http://www.statsmodels.org/stable/glm.html#technical-documentation) provides a helpful summary about the model, so it is preferable for this example.

## 5.4 Fit a linear regression model.

```{python, echo=4:11}
import pandas as pd
import numpy as np
tips = pd.read_csv('/Users/tips.csv')
# Fit a linear regression model of tip by total_bill
from sklearn.linear_model import LinearRegression

# If your data has one feature, you need to reshape the 1D array
linreg = LinearRegression()
linreg.fit(tips["total_bill"].values.reshape(-1,1), tips["tip"])
print(linreg.coef_)
print(linreg.intercept_)
```
[LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

***

# 6 Supervised Machine Learning

## 6.1 Fit a logistic regression model on training data and assess against testing data.

### a) Fit a logistic regression model on training data.
```{python, echo=4:16}
import pandas as pd
import numpy as np
import statsmodels.api as sm
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')

train["fifteen"] = 0.15 * train["total_bill"]
train["greater15"] = np.where(train["tip"] > train["fifteen"], 1, 0)
test["fifteen"] = 0.15 * test["total_bill"]
test["greater15"] = np.where(test["tip"] > test["fifteen"], 1, 0)

logreg = sm.formula.glm("greater15 ~ total_bill", 
                       family=sm.families.Binomial(), 
                       data=train).fit()
print(logreg.summary())
```

### b) Assess the model against the testing data.
```{python, echo=12:23}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')
train["fifteen"] = 0.15 * train["total_bill"]
train["greater15"] = np.where(train["tip"] > train["fifteen"], 1, 0)
test["fifteen"] = 0.15 * test["total_bill"]
test["greater15"] = np.where(test["tip"] > test["fifteen"], 1, 0)
import statsmodels.api as sm
logreg = sm.formula.glm("greater15 ~ total_bill", family=sm.families.Binomial(), 
                     data=train).fit()
# Prediction on testing data
predictions = logreg.predict(test["total_bill"])
predY = np.where(predictions < 0.5, 0, 1)

# If the prediction probability is less than 0.5, classify this as a 0
# and otherwise classify as a 1.  This isn't the best method -- a better 
# method would be randomly assigning a 0 or 1 when a probability of 0.5 
# occurrs, but this insures that results are consistent 

# Determine how many were correctly classified 
Results = np.where(predY == test["greater15"], "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
A logistic regression model can be implemented using [sklearn](#SKLEARN), however [statsmodels.api](http://www.statsmodels.org/stable/glm.html#technical-documentation) provides a helpful summary about the model, so it is preferable for this example.

## 6.2 Fit a linear regression model on training data and assess against testing data.

### a) Fit a linear regression model on training data.
```{python, echo=4:12}
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

# Fit a linear regression model
linreg = LinearRegression()
linreg.fit(train.drop(["Target"], axis = 1), train["Target"])
print(linreg.coef_)
print(linreg.intercept_)
```

### b) Assess the model against the testing data.
```{python, echo=8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.linear_model import LinearRegression
linreg = LinearRegression()
linreg.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
prediction = pd.DataFrame()
prediction["predY"] = linreg.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
prediction["sq_diff"] = (prediction["predY"] - test["Target"])**2
print(np.mean(prediction["sq_diff"]))
```
[LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

## 6.3 Fit a decision tree model on training data and assess against testing data.

### a) Fit a decision tree classification model.

#### i) Fit a decision tree classification model on training data and determine variable importance.
```{python, echo=3:20}
import pandas as pd
import numpy as np
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

from sklearn.tree import DecisionTreeClassifier

# random_state is used to specify a seed for a random integer so that the 
# results are reproducible
treeMod = DecisionTreeClassifier(random_state=29)
treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = treeMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo=8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.tree import DecisionTreeClassifier
treeMod = DecisionTreeClassifier(random_state=29)
treeMod = treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
predY = treeMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == predY, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
[DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)

### b) Fit a decision tree regression model.

#### i) Fit a decision tree regression model on training data and determine variable importance.
```{python, echo=3:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

from sklearn.tree import DecisionTreeRegressor

treeMod = DecisionTreeRegressor(random_state=29)
treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = treeMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo=8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.tree import DecisionTreeRegressor
treeMod = DecisionTreeRegressor(random_state=29)
treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
prediction = pd.DataFrame()
prediction["predY"] = treeMod.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
prediction["sq_diff"] = (prediction["predY"] - test["Target"])**2
print(np.mean(prediction["sq_diff"]))
```
[DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)

## 6.4 Fit a random forest model on training data and assess against testing data.

### a) Fit a random forest classification model.

#### i) Fit a random forest classification model on training data and determine variable importance.
```{python, echo = 3:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

from sklearn.ensemble import RandomForestClassifier

rfMod = RandomForestClassifier(random_state=29)
rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = rfMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.ensemble import RandomForestClassifier
rfMod = RandomForestClassifier(random_state=29)
rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
predY = rfMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == predY, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)

### b) Fit a random forest regression model.

#### i) Fit a random forest regression model on training data and determine variable importance.
```{python, echo = 3:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

from sklearn.ensemble import RandomForestRegressor

rfMod = RandomForestRegressor(random_state=29)
rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = rfMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.

```{python, echo = 8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.ensemble import RandomForestRegressor
rfMod = RandomForestRegressor(random_state=29)
rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
prediction = pd.DataFrame()
prediction["predY"] = rfMod.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
prediction["sq_diff"] = (test["Target"] - prediction["predY"])**2
print(prediction["sq_diff"].mean())
```
[RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)

## 6.5 Fit a gradient boosting model on training data and assess against testing data.

### a) Fit a gradient boosting classification model.

#### i) Fit a gradient boosting classification model on training data and determine variable importance.
```{python, echo = 3:22}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

from sklearn.ensemble import GradientBoostingClassifier

# n_estimators = total number of trees to fit which is analogous to the 
# number of iterations
# learning_rate = shrinkage or step-size reduction, where a lower 
# learning rate requires more iterations
gbMod = GradientBoostingClassifier(random_state = 29, learning_rate = .01, 
                                  n_estimators = 2500)
gbMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = gbMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.ensemble import GradientBoostingClassifier
gbMod = GradientBoostingClassifier(random_state = 29, learning_rate = .01, n_estimators = 2500)
gbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
predY = gbMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == predY, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
[GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)

### b) Fit a gradient boosting regression model.

#### i) Fit a gradient boosting regression model on training data and determine variable importance.
```{python, echo = 3:18}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

from sklearn.ensemble import GradientBoostingRegressor

gbMod = GradientBoostingRegressor(random_state = 29, learning_rate = .01, 
                                  n_estimators = 2500)
gbMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = gbMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.ensemble import GradientBoostingRegressor
gbMod = GradientBoostingRegressor(random_state = 29, learning_rate = .01, n_estimators = 2500)
gbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
prediction = pd.DataFrame()
prediction["predY"] = gbMod.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
prediction["sq_diff"] = (test["Target"] - prediction["predY"])**2
print(prediction["sq_diff"].mean())
```
[GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)

## 6.6 Fit an extreme gradient boosting model on training data and assess against testing data.

### a) Fit an extreme gradient boosting classification model on training data and assess against testing data.

#### i) Fit an extreme gradient boosting classification model on training data.
```{python, eval = FALSE}
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

from xgboost import XGBClassifier

# Fit XGBClassifier model on training data
xgbMod = XGBClassifier(seed = 29, learning_rate = 0.01, 
                       n_estimators = 2500)
xgbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from xgboost import XGBClassifier
xgbMod = XGBClassifier(seed = 29, learning_rate = 0.01, n_estimators = 2500)
xgbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
predY = xgbMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == predY, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
[xgboost](#XGBOOST)

### b) Fit an extreme gradient boosting regression model on training data and assess against testing data.

#### i) Fit an extreme gradient boosting regression model on training data.
```{python, eval = FALSE}
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

from xgboost import XGBRegressor

# Fit XGBRegressor model on training data
xgbMod = XGBRegressor(seed = 29, learning_rate = 0.01, 
                      n_estimators = 2500)
xgbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from xgboost import XGBRegressor
xgbMod = XGBRegressor(seed = 29, learning_rate = 0.01, n_estimators = 2500)
xgbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
prediction = pd.DataFrame()
prediction["predY"] = xgbMod.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
prediction["sq_diff"] = (test["Target"] - prediction["predY"])**2
print(prediction["sq_diff"].mean())
```
[xgboost](#XGBOOST) 

## 6.7 Fit a support vector model on training data and assess against testing data.

### a) Fit a support vector classification model.

#### i) Fit a support vector classification model on training data.

Note: In implementation scaling should be used.
```{python, eval = FALSE}
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

# Fit a support vector classification model
from sklearn.svm import SVC
svMod = SVC(random_state = 29, kernel = 'linear')
svMod.fit(train.drop(["Target"], axis = 1), train["Target"])
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.svm import SVC
svMod = SVC(random_state = 29, kernel = 'linear')
svMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
prediction = svMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == prediction, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
[SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)

### b) Fit a support vector regression model.

#### i) Fit a support vector regression model on training data.

Note: In implementation scaling should be used.
```{python, eval = FALSE}
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

# Fit a support vector regression model
from sklearn.svm import SVR
svMod = SVR()
svMod.fit(train.drop(["Target"], axis = 1), train["Target"])
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.svm import SVR
svMod = SVR()
svMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
prediction = pd.DataFrame()
prediction["predY"] = svMod.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
prediction["sq_diff"] = (test["Target"] - prediction["predY"])**2
print(prediction["sq_diff"].mean())
```
[SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)

## 6.8 Fit a neural network model on training data and assess against testing data.

### a) Fit a neural network classification model.

#### i) Fit a neural network classification model on training data.
```{python, eval = FALSE}
# Notice we are using new data sets
train = pd.read_csv('/Users/digits_train.csv')
test = pd.read_csv('/Users/digits_test.csv')

# Fit a neural network classification model on training data
from sklearn.neural_network import MLPClassifier
nnMod = MLPClassifier(max_iter = 200, hidden_layer_sizes=(100,), 
                      random_state = 29)
nnMod.fit(train.drop(["Target"], axis = 1), train["Target"])
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:15}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/digits_train.csv')
test = pd.read_csv('/Users/digits_test.csv')
from sklearn.neural_network import MLPClassifier
nnMod = MLPClassifier(max_iter = 200, hidden_layer_sizes=(100,), random_state = 29)
nnMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
predY = nnMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified

from sklearn.metrics import confusion_matrix

print(confusion_matrix(test["Target"], predY))
```
[MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) | [confusion_matrix()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)

### b) Fit a neural network regression model.

#### i) Fit a neural network regression model on training data.
```{python, eval = FALSE}
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

# Scale input data
from sklearn.preprocessing import StandardScaler

train_features = train.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(train_features))
train_scaled = scaler.transform(np.array(train_features))
train_scaled = pd.DataFrame(train_scaled)

test_features = test.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(test_features))
test_scaled = scaler.transform(np.array(test_features))
test_scaled = pd.DataFrame(test_scaled)

# Fit neural network regression model, dividing target by 50 for scaling
from sklearn.neural_network import MLPRegressor
nnMod = MLPRegressor(max_iter = 250, random_state = 29, solver = 'lbfgs')
nnMod = nnMod.fit(train_scaled, train["Target"] / 50)
```

#### ii) Assess the model against testing data.
```{python, echo = 17:23}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.preprocessing import StandardScaler
train_features = train.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(train_features))
train_scaled = scaler.transform(np.array(train_features))
train_scaled = pd.DataFrame(train_scaled)
test_features = test.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(test_features))
test_scaled = scaler.transform(np.array(test_features))
test_scaled = pd.DataFrame(test_scaled)
from sklearn.neural_network import MLPRegressor
nnMod = MLPRegressor(max_iter = 250, random_state = 29, solver = 'lbfgs')
nnMod = nnMod.fit(train_scaled, train["Target"] / 50)
# Prediction on testing data, remembering to multiply by 50
prediction = pd.DataFrame()
prediction["predY"] = nnMod.predict(test_scaled)*50

# Determine mean squared error
prediction["sq_diff"] = (test["Target"] - prediction["predY"])**2
print(prediction["sq_diff"].mean())
```
[preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html) | [MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)

***

# 7 Unsupervised Machine Learning

## 7.1 KMeans Clustering

```{python, echo = 3:13}
import pandas as pd
import numpy as np
iris = pd.read_csv('/Users/iris.csv')
iris["Species"] = np.where(iris["Target"] == 0, "Setosa", 
                           np.where(iris["Target"] == 1, "Versicolor", 
                                    "Virginica"))
features = pd.concat([iris["PetalLength"], iris["PetalWidth"], 
                     iris["SepalLength"], iris["SepalWidth"]], axis = 1)

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters = 3, random_state = 29).fit(features)

print(pd.crosstab(index = iris["Species"], columns = kmeans.labels_))
```
[KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

## 7.2 Spectral Clustering

```{python, echo = 6:11}
import pandas as pd
import numpy as np
iris = pd.read_csv('/Users/iris.csv')
iris["Species"] = np.where(iris["Target"] == 0, "Setosa", np.where(iris["Target"] == 1, "Versicolor", "Virginica"))
features = pd.concat([iris["PetalLength"], iris["PetalWidth"], iris["SepalLength"], iris["SepalWidth"]], axis = 1)
from sklearn.cluster import SpectralClustering

spectral = SpectralClustering(n_clusters = 3, 
                              random_state = 29).fit(features)

print(pd.crosstab(index = iris["Species"], columns = spectral.labels_))
```
[SpectralClustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering)

## 7.3 Ward Hierarchical Clustering

```{python, echo = 6:10}
import pandas as pd
import numpy as np
iris = pd.read_csv('/Users/iris.csv')
iris["Species"] = np.where(iris["Target"] == 0, "Setosa", np.where(iris["Target"] == 1, "Versicolor", "Virginica"))
features = pd.concat([iris["PetalLength"], iris["PetalWidth"], iris["SepalLength"], iris["SepalWidth"]], axis = 1)
from sklearn.cluster import AgglomerativeClustering

aggl = AgglomerativeClustering(n_clusters = 3).fit(features)

print(pd.crosstab(index = iris["Species"], columns = aggl.labels_))
```
[AgglomerativeClustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering)

## 7.4 DBSCAN

```{python, echo = 6:10}
import pandas as pd
import numpy as np
iris = pd.read_csv('/Users/iris.csv')
iris["Species"] = np.where(iris["Target"] == 0, "Setosa", np.where(iris["Target"] == 1, "Versicolor", "Virginica"))
features = pd.concat([iris["PetalLength"], iris["PetalWidth"], iris["SepalLength"], iris["SepalWidth"]], axis = 1)
from sklearn.cluster import DBSCAN

dbscan = DBSCAN().fit(features)

print(pd.crosstab(index = iris["Species"], columns = dbscan.labels_))
```
[DBCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN)

## 7.5 Self-organizing map

```{python, eval = FALSE}
from pyclustering.nnet import som

sm = som.som(4,4)

sm.train(features.as_matrix(), 100)

sm.show_distance_matrix()
```
Output:
![output](u-matrix.png)

[pyclustering](#PYCLUSTERING)

***

# 8 Forecasting

## 8.1 Fit an ARIMA model to a timeseries.

### a) Plot the timeseries.
```{python, eval = FALSE}
# Read in new data set
air = pd.read_csv('/Users/air.csv')

air["DATE"] = pd.to_datetime(air["DATE"], infer_datetime_format = True)
air.index = air["DATE"].values

plt.plot(air.index, air["AIR"])
plt.show()
```
Output:
![output](ARIMASeriess.png)

[to_datetime()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html)

### b) Fit an ARIMA model and predict 2 years (24 months).
```{python, eval = FALSE}
import pyflux as pf

# ar = 12 is necessary to indicate the seasonality of data
model = pf.ARIMA(data = air, ar = 12, ma = 1, integ = 0, target = 'AIR', 
                 family = pf.Normal())
x = model.fit("MLE")
model.plot_predict(h = 24)
```
Output:
![output](ARIMAForecasts.png)

[PyFlux](#PYFLUX)

## 8.2 Fit a Simple Exponential Smoothing model to a timeseries.

### a) Plot the timeseries.
```{python, eval = FALSE}
# Read in new data set
usecon = pd.read_csv('/Users/usecon.csv')

petrol = usecon["PETROL"]

plt.plot(petrol)
plt.show()
```
Output:
![output](PetrolSeriess.png)

### b) Fit a Simple Exponential Smoothing model, predict 2 years (24 months) out and plot predictions.

Currently, there is not a good package in Python to fit a simple exponential smoothing model.  The formula for fitting an exponential smoothing model is not difficult, so we can do it by creating our own functions in Python.

The simplest form of exponential smoothing is given by, where $t > 0$: 

$$Eq1: s_0 = x_0$$

$$Eq2: s_t = \alpha x_t + (1 - \alpha)s_{t-1}$$
Therefore, we can implement a simple exponential smoothing model as follows:
```{python, eval = FALSE}
def simple_exp_smoothing(data, alpha, n_preds):
    # Eq1:
    output = [data[0]]
    # Smooth given data plus we want to predict 24 units 
    # past the end
    for i in range(1, len(data) + n_preds):
        # Eq2:
        if (i < len(data)):
            output.append(alpha * data[i] + (1 - alpha) * data[i-1])
        else:
            output.append(alpha * output[i-1] + (1 - alpha) * output[i-2])
    return output
    
pred = simple_exp_smoothing(petrol, 0.9999, 24)

plt.plot(pd.DataFrame(pred), color = "red")
plt.plot(petrol, color = "blue")
plt.show()
```
Output:
![output](PetrolForecasts.png)

[Basis for code](https://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/)

## 8.3 Fit a Holt-Winters model to a timeseries.

### a) Plot the timeseries.
```{python, eval = FALSE}
vehicle = usecon["VEHICLE"]

plt.plot(vehicle)
plt.show()
```
Output:
![output](VehiclesSeriess.png)

### b) Fit a Holt-Winters additive model, predict 2 years (24 months) out and plot predictions.
Currently, there is not a good package in Python to fit a Holt-Winters additive model.  The formula for fitting a Holt-Winters additive model is not difficult, so we can do it by creating our own functions in Python.

The following is an implementation of the Holt-Winters additive model given at [triple exponential smoothing code](https://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/).

```{python, eval = FALSE}
def initial_trend(series, slen):
    sum = 0.0
    for i in range(slen):
        sum += float(series[i+slen] - series[i]) / slen
    return sum / slen
def initial_seasonal_components(series, slen):
    seasonals = {}
    season_averages = []
    n_seasons = int(len(series)/slen)
    # compute season averages
    for j in range(n_seasons):
        season_averages.append(sum(series[slen*j:slen*j+slen])/float(slen))
    # compute initial values
    for i in range(slen):
        sum_of_vals_over_avg = 0.0
        for j in range(n_seasons):
            sum_of_vals_over_avg += series[slen*j+i]-season_averages[j]
        seasonals[i] = sum_of_vals_over_avg/n_seasons
    return seasonals
def triple_exponential_smoothing_add(series, slen, alpha, beta, gamma, 
                                     n_preds):
    result = []
    seasonals = initial_seasonal_components(series, slen)
    for i in range(len(series)+n_preds):
        if i == 0: # initial values
            smooth = series[0]
            trend = initial_trend(series, slen)
            result.append(series[0])
            continue
        if i >= len(series): # we are forecasting
            m = i - len(series) + 1
            result.append((smooth + m*trend) + seasonals[i%slen])
        else:
            val = series[i]
            last_smooth, smooth = smooth, alpha*(val-seasonals[i%slen]) 
                                                 + (1-alpha)*(smooth+trend)
            trend = beta * (smooth-last_smooth) + (1-beta)*trend
            seasonals[i%slen] = gamma*(val-smooth) + 
                                (1-gamma)*seasonals[i%slen]
            result.append(smooth+trend+seasonals[i%slen])
    return result
    
add_preds = triple_exponential_smoothing_add(vehicles, 12, 0.5731265, 0, 
                                             0.7230956, 24)

plt.plot(pd.DataFrame(add_preds), color = "red")
plt.plot(vehicles, color = "blue")
plt.show()
```
Output:
![output](VehiclesForecasts1.png)

## 8.4 Fit a Facebook Prophet forecasting model to a timeseries.
```{python, eval = FALSE}
from fbprophet import Prophet

air = pd.read_csv("/Users/air.csv")

air_df = pd.DataFrame()

air_df["ds"] = pd.to_datetime(air["DATE"], infer_datetime_format = True)
air_df["y"] = air["AIR"]

m = Prophet(yearly_seasonality = True, weekly_seasonality = False)

m.fit(air_df)

future = m.make_future_dataframe(periods = 24, freq = "M")

forecast = m.predict(future)

m.plot(forecast)
```

Output:
![output](prophet_air.png)

[Facebook Prophet Python API](#FBPROPHET)

***

# 9 Model Evaluation & Selection

## 9.1 Evaluate the accuracy of regression models.

### a) Evaluation on training data.
```{python, echo=3:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

# Random Forest Regression Model
from sklearn.ensemble import RandomForestRegressor
rfMod = RandomForestRegressor(random_state=29)
rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Evaluation on training data
predY = rfMod.predict(train.drop(["Target"], axis = 1))

# Determine coefficient of determination score
from sklearn.metrics import r2_score
r2_rf = r2_score(train["Target"], predY)
print("Random forest regression model r^2 score (coefficient of determination): %f" % r2_rf)
```

### b) Evaluation on testing data.
```{python, echo=10:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
rfMod = RandomForestRegressor(random_state=29)
rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])
from sklearn.metrics import r2_score
# Random Forest Regression Model (rfMod)

# Evaluation on testing data
predY = rfMod.predict(test.drop(["Target"], axis = 1))

# Determine coefficient of determination score
r2_rf = r2_score(test["Target"], predY)
print("Random forest regression model r^2 score (coefficient of determination): %f" % r2_rf)
```
[RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)

The sklearn metric [r2_score](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination) is only one option for assessing a regression model.  Please go [here](http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) for more information about other sklearn regression metrics.

## 9.2 Evaluate the accuracy of classification models.

### a) Evaluation on training data.
```{python, echo = 3:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/digits_train.csv')
test = pd.read_csv('/Users/digits_test.csv')

# Random Forest Classification Model
from sklearn.ensemble import RandomForestClassifier
rfMod = RandomForestClassifier(random_state=29)
rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Evaluation on training data
predY = rfMod.predict(train.drop(["Target"], axis = 1))

# Determine accuracy score
from sklearn.metrics import accuracy_score
accuracy_rf = accuracy_score(train["Target"], predY)
print("Random forest model accuracy: %f" % accuracy_rf)
```

### b) Evaluation on testing data.
```{python, echo = 9:16}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/digits_train.csv')
test = pd.read_csv('/Users/digits_test.csv')
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rfMod = RandomForestClassifier(random_state=29)
rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Random Forest Classification Model (rfMod)

# Evaluation on testing data
predY = rfMod.predict(test.drop(["Target"], axis = 1))

# Determine accuracy score
accuracy_rf = accuracy_score(test["Target"], predY)
print("Random forest model accuracy: %f" % accuracy_rf)
```
[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)

Note: The sklearn metric [accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) is only one option for assessing a classification model.  Please go [here](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) for more information about other sklearn classification metrics.

## 9.3 Evaluation with cross validation.

### a) KFold
```{python, echo = 3:18}
import pandas as pd
import numpy as np
# Notice we are using a new data set that need to be read into the 
# environment 
breastcancer = pd.read_csv('/Users/breastcancer.csv')

from sklearn import model_selection
from sklearn.ensemble import RandomForestClassifier

X = breastcancer.drop(["Target"], axis = 1)
Y = breastcancer["Target"]

kfold = model_selection.KFold(n_splits = 5, random_state = 29)
rfMod = RandomForestClassifier(random_state = 29)
results = model_selection.cross_val_score(rfMod, X, Y, cv = kfold)

print("Accuracy: %.2f%% +/- %.2f%%" % (results.mean()*100, 
                                       results.std()*100))
```
[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) | [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)

### b) ShuffleSplit

```{python, echo = 8:13}
import pandas as pd
import numpy as np
breastcancer = pd.read_csv('/Users/breastcancer.csv')
from sklearn import model_selection
from sklearn.ensemble import RandomForestClassifier
X = breastcancer.drop(["Target"], axis = 1)
Y = breastcancer["Target"]
shuffle = model_selection.ShuffleSplit(n_splits = 5, random_state = 29)
rfMod = RandomForestClassifier(random_state = 29)
results = model_selection.cross_val_score(rfMod, X, Y, cv = shuffle)

print("Accuracy: %.2f%% +/- %.2f%%" % (results.mean()*100, 
                                       results.std()*100))
```
[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) | [ShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html)

***

\newpage

# Appendix

## 1 Built-in Python Data Types

* [Boolean](https://docs.python.org/2/library/stdtypes.html#boolean-values)

#### Numeric types
* [int](https://docs.python.org/3/library/functions.html#int)
* [long](https://docs.python.org/2/library/functions.html#long)
* [float](https://docs.python.org/2/library/functions.html#float)
* [complex](https://docs.python.org/2/library/functions.html#complex)

#### Sequences
* [str](#str)
* [bytes](#BYTE)
* [byte array](#BYTE)
* [list](#LIST)
* [tuple](#LIST)

#### Sets
* [set](#SET)
* [frozen set](#SET)

#### Mapping:
* [dictionary](#dict)

## 2 Python Plotting Packages

#### [Bokeh](http://bokeh.pydata.org/en/latest/) {#bokeh}
A Python package which is useful for interactive visualizations and is optimized for web browser presentations.

#### [PyPlot](https://matplotlib.org/api/pyplot_api.html) {#PYPLOT}
A Python package which is useful for data plotting and visualization.

#### [Seaborn](https://seaborn.pydata.org/) {#SEABORN}
A Python package which is useful for data plotting and visualization.  In particular, Seaborn includes tools for drawing attractive statistical graphics.

## 3 Python packages used in this tutorial

#### [pandas](http://pandas.pydata.org/) {#PANDAS}
Working with data structures and performing data analysis

#### [NumPy](http://www.numpy.org/) {#NUMPY}
Scientific and mathematical computing

#### [re](https://docs.python.org/2/library/re.html#module-re) {#re}
Regular expressions

#### [Decimal](https://docs.python.org/2/library/decimal.html) {#DECIMAL}
Tools for decimal [floating point](#float) arithmetic

#### [sklearn](http://scikit-learn.org/stable/) {#SKLEARN}
scikit-learn, or more commonly known as sklearn, is useful for basic and advanced data mining, machine learning, and data analysis.  sklearn includes tools for classification, regression, clustering, dimensionality reduction, model selection, and data pre-processing.

#### [statsmodels.api](http://www.statsmodels.org/stable/index.html) {#STATSMODELS}
Tools for the estimation of many different statistical models

#### [xgboost](http://xgboost.readthedocs.io/en/latest/python/python_intro.html) {#XGBOOST}
Extreme gradient boosting models

#### [pyclustering](http://pythonhosted.org/pyclustering/) {#PYCLUSTERING}
Tools for clustering input data

#### [PyFlux](http://www.pyflux.com/docs/) {#PYFLUX}
Tools for time series analysis and prediction

#### [FBProphet](https://facebookincubator.github.io/prophet/docs/quick_start.html#python-api) {#FBPROPHET}
Tools for forecasting using the Facebook Prophet model

***

\newpage

# Alphabetical Index

## [Array](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)

A [NumPy](#NUMPY) array is a data type in which the elements of the array are all of the same type.  Please see the following example of array creation and access:
```{python}
import numpy as np
my_array = np.array([1, 2, 3, 4])
print(my_array)
```
```{python, echo=3}
import numpy as np
my_array = np.array([1, 2, 3, 4])
print(my_array[3])
```

## Bytes & Byte arrays {#BYTE}
A [byte](https://docs.python.org/3.1/library/functions.html#bytes) is a sequence of integers which is immutable, whereas a [byte array](https://docs.python.org/3.1/library/functions.html#bytearray) is its mutable counterpart.  

## [Data Frame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) {#DataFrame}
A two-dimensional tabular structure with labeled axes (rows and columns), where data observations are represented by rows and data variables are represented by columns.

## [Dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries) {#DICT}
An associative array which is indexed by keys which map to values.  Therefore, a dictionary is an unordered set of key:value pairs where each key is unique.  Please see the following example of dictionary creation and access:
```{python}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
for_dict = pd.concat([student["Name"], student["Age"]], axis = 1)
class_dict = for_dict.set_index('Name').T.to_dict('list')
print(class_dict.get('James'))
```

## [List](https://www.tutorialspoint.com/python/python_lists.htm) {#LIST}
A sequence of comma-separated objects that need not be of the same type.  Please see the following example of list creation and access:
```{python}
list1 = ['item1', 102]
print(list1)
```
```{python, echo = 2}
list1 = ['item1', 102]
print(list1[1])
```
Python also has what are known as ["Tuples"](https://www.tutorialspoint.com/python/python_tuples.htm), which are immutable lists created in the same way as lists, except with paranthesis instead of brackets.

## [Series](https://pandas.pydata.org/pandas-docs/stable/dsintro.html)

A one-dimensional data frame.  Please see the following example of Series creation and access:
```{python}
import pandas as pd
my_array = pd.Series([1, 3, 5, 9])
print(my_array)
```
```{python, echo = 3}
import pandas as pd
my_array = pd.Series([1, 3, 5, 9])
print(my_array[1])
```

## Sets & Frozen Sets {#SET}
A set is a unordered collection of immutable objects.  The difference between a [set and a frozen set](http://www.python-course.eu/sets_frozensets.php) is that the former is mutable, while the latter is immutable.  Please see the following example of set and frozen set creation and access:
```{python}
s = set(["1", "2", "3"])
# s is a set, which means you can add or delete elements from s
print(s)
```

```{python, echo = 2:3}
s = set(["1", "2", "3"])
s.add("4")
print(s)
```

```{python}
fs = frozenset(["1", "2", "3"])
# fs is a frozenset, which means you cannot add or delete elements from fs
print(fs)
```

## [str](https://www.tutorialspoint.com/python/python_strings.htm) {#str}
A list of characters, though characters are not a type in Python, but rather a string of length 1.  Strings are indexable like arrays.  Please see the following example of String creation and access:
```{python}
s = 'My first string!'
print(s)
```
```{python, echo = 2}
s = 'My first string!'
print(s[5])
```

***

For more information on Python packages and functions, along with helpful examples, please see [Python](https://www.python.org/).


