---
title: "Data Science Using Python, SAS, & R"
author: "Elaine Kearney"
subtitle: A Rosetta Stone for Analytical Languages
output:
  html_document:
    df_print: paged
    highlight: tango
    toc: yes
    toc_float: yes
  word_document:
    highlight: tango
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, highlight = TRUE)
```

# R Tutorial

Welcome to the R tutorial version of *Data Science Using Python, SAS, & R: A Rosetta Stone for Analytical Languages*.  This tutorial includes examples of common data science tasks, organized in the same way across 3 data science languages.  Before beginning this tutorial, please check to make sure you have R 3.3.1 installed (this is not required, but this was the release used to generate the following examples).  Also, the following R packages are used throughout this tutorial.  You may not need all of the following packages to fit your specific needs, but they are listed below, and also in Appendix Section 2 with more detail, for your information:

[gdata](#GDATA) | [rjson](#RJSON) | [ggplot2](#ggplot2) | [dplyr](#DPLYR) | [tree](#TREE) | [randomForest](#RANDOM) | [gbm](#GBM) | [xgboost](#XGBOOST) | [e1071](#e1071) | [RSNNS](#RSNNS) | [caret](#CARET) | [kernlab](#KERNLAB) | [dbscan](#DBSCAN) | [kohonen](#KOHONEN) | [forecast](#FORECAST) | [prophet](#PROPHET) 

To install R packages you need to run the following in the R console:

```{r, eval = FALSE}
install.packages("name_of_package")
```

Note: In R, comments are indicated in code with a "#" character, and arrays and matrices begin with index 1.  Also, "<-" and "=" can be used interchangeably.

Now let's get started!

***

# 1 Reading in Data and Basic Statistical Functions

## 1.1 Read in the data.

### a) Read the data in as a .csv file.

```{r student1}
student <- read.csv('/Users/class.csv')
```

[read.csv()](https://www.rdocumentation.org/packages/utils/versions/3.4.0/topics/read.table)

### b) Read the data in as a .xls file.

```{r, eval=FALSE}
# call the gdata package
library(gdata)

student_xls <- read.xls('/Users/class.xls', 1)
```

[gdata](#GDATA) | [read.xls()](https://www.rdocumentation.org/packages/gdata/versions/2.18.0/topics/read.xls)

###c) Read the data in as a .json file.

There is more code involved in reading a .json file into R so it becomes a proper [data frame](#DataFrame).  Also, this code is specific for a certain .json format, so you may have to change it to fix your needs.
```{r, eval = FALSE}
# call the rjson package
library(rjson)

temp <- fromJSON(file = '/Users/class.json')
temp <- do.call('rbind', temp)
temp <- data.frame(temp, stringsAsFactors = TRUE)
temp <- transform(temp, Name=unlist(Name), Sex=unlist(Sex), Age=unlist(Age), 
                  Height=unlist(Height), Weight=unlist(Weight))
temp$Name <- as.factor(temp$Name)
temp$Sex <- as.factor(temp$Sex)
temp$Age <- as.integer(temp$Age)

student_json <- temp
```

[rjson](#RJSON) | [fromJSON()](https://www.rdocumentation.org/packages/rjson/versions/0.2.15/topics/fromJSON)

## 1.2 Find the dimensions of the data set.

The shape of an R [data frame](#DataFrame) is available by calling the [dim()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/dim) function, with the data name as an argument.
```{r}
dim(student)
```

## 1.3 Find basic information about the data set.

Information about an R [data frame](#DataFrame) is available by calling the [str()](https://www.rdocumentation.org/packages/utils/versions/3.4.0/topics/str) function, with the data name as an argument.
```{r structure}
str(student)
```

## 1.4 Look at the first 5 (last 5) observations.

The first 5 observations of a [data frame](#DataFrame) are available by calling the [head()](https://www.rdocumentation.org/packages/utils/versions/3.4.0/topics/head) function, with the data name as an argument.  By default, head() returns 4 observations, but we can alter the function to return 5 observations in the way shown below (*n*= ).  The [tail()](https://www.rdocumentation.org/packages/utils/versions/3.4.0/topics/head) function is analogous and returns the last observations.
```{r head}
head(student, n=5)
```

## 1.5 Calculate means of numeric variables.

```{r mean, include=TRUE}
# We must apply the is.numeric() function to the data set which returns a 
# matrix of booleans that we then use to subset the data set to return 
# only numeric variables  

# Then we can use the colMeans() function to return the means of 
# column variables
colMeans(student[sapply(student, is.numeric)])
```

[colMeans()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/colSums) | [sapply()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/lapply) |  [is.numeric](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/numeric)

## 1.6 Compute summary statistics of the data set.

Summary statistics of a [data frame](#DataFrame) are available by calling the [summary()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/summary) function, with the data name as an argument.
```{r summary}
summary(student)
```

## 1.7 Descriptive statistics functions applied to columns of the data set.

```{r stats}
# Notice the subsetting of student with the "$" character 
sd(student$Weight)
sum(student$Weight)
length(student$Weight)
max(student$Weight)
min(student$Weight)
median(student$Weight)
```

## 1.8 Produce a one-way table to describe the frequency of a variable.

### a) Produce a one-way table of a discrete variable.
```{r}
table(student$Age)
```

### b) Produce a one-way table of a categorical variable.
```{r}
table(student$Sex)
```

[table()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/table)

## 1.9 Produce a two-way table to visualize the frequency of two categorical (or discrete) variables.

```{r two-way}
table(student$Age, student$Sex)
```

[table()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/table)

## 1.10 Select a subset of the data that meets a certain criterion.

```{r subset}
# The "," character tells R to select all columns of the data set
females <- student[which(student$Sex == 'F'), ]
head(females, n=5)
```

[which()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/which)

## 1.11 Determine the correlation between two continuous variables.

```{r}
height_weight <- subset(student, select = c(Height, Weight))
cor(height_weight, method = "pearson")
```

[subset()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/subset) | [cor()](https://www.rdocumentation.org/packages/WGCNA/versions/1.51/topics/cor)

***

\newpage

# 2 Basic Graphing and Plotting Functions

## 2.1 Visualize a single continuous variable by producing a histogram.

```{r histogram}
# Setting student$Weight to a new variable “Weight” cleans up the labeling of 
# the histogram 
Weight <- student$Weight
hist(Weight)
```

[hist()](https://www.rdocumentation.org/packages/graphics/versions/3.4.0/topics/hist)

## 2.2 Visualize a single continuous variable by producing a boxplot.

```{r boxplot}
# points(mean(Weight)) tells R to plot the mean on the boxplot 
boxplot(Weight, ylab="Weight")
points(mean(Weight))
```

[boxplot()](https://www.rdocumentation.org/packages/graphics/versions/3.4.0/topics/boxplot) | [points()](https://www.rdocumentation.org/packages/graphics/versions/3.4.0/topics/points)

## 2.3 Visualize two continuous variables by producing a scatterplot.

```{r scatter}
Height <- student$Height
# Notice here you specify the x variable, followed by the y variable 
plot(Height, Weight)
```

[plot()](https://www.rdocumentation.org/packages/graphics/versions/3.4.0/topics/plot)

## 2.4 Visualize a relationship between two continuous variables by producing a scatterplot and a plotted line of best fit.

```{r scatterline}
plot(Height, Weight)

# lm() models Weight as a function of Height and returns the parameters 
# of the line of best fit
model <- lm(Weight~Height)
coeff <- coef(model)
intercept <- as.matrix(coeff[1])[1]
slope <- as.matrix(coeff[2])[1]

# abline() prints the line of best fit 
abline(lm(Weight~Height))

# text() prints the equation of the line of best fit, with the first 
# two arguments specifying the x and y location, respectively, of where 
# the text should be printed on the graph 
text(55, 140, bquote(Line: y == .(slope) * x + .(intercept)))
```

[lm()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/lm) | [coef()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/coef) | [as.matrix()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/matrix) | [abline()](https://www.rdocumentation.org/packages/graphics/versions/3.4.0/topics/abline) | [text()](https://www.rdocumentation.org/packages/graphics/versions/3.4.0/topics/text) | [bquote()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/bquote)

## 2.5 Visualize a categorical variable by producing a bar chart.

```{r bar}
counts <- table(student$Sex)

# beside = TRUE indicates to print the bars side by side instead of on top of 
# each other 
# names.arg indicates which names to use to label the bars 
barplot(counts, beside=TRUE, ylab= "Frequency", xlab= "Sex", 
        names.arg=names(counts))
```

[barplot()](https://www.rdocumentation.org/packages/graphics/versions/3.4.0/topics/barplot) | [names()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/names)

## 2.6 Visualize a continuous variable, grouped by a categorical variable, using side-by-side boxplots.

### a) Simple side-by-side boxplot without color.
```{r contcat}
# Subset data set to return only female weights, and then only male weights 
Female_Weight <- student[which(student$Sex == 'F'), "Weight"]
Male_Weight <- student[which(student$Sex == 'M'), "Weight"]

# Find the mean of both arrays 
means <- c(mean(Female_Weight), mean(Male_Weight))
 
# Syntax indicates Weight as a function of Sex 
boxplot(student$Weight ~ student$Sex, ylab= "Weight", xlab= "Sex")

# Plot means on boxplots in blue 
points(means, col= "blue")
```

### b) More advanced side-by-side boxplot with color.
```{r, message = FALSE, warning = FALSE}
# call the ggplot2 package
library(ggplot2)

student$Sex <- factor(student$Sex, levels = c("F","M"), 
                      labels = c("Female", "Male"))
ggplot(data = student, aes(x = Sex, y = Weight, fill = Sex)) + 
  geom_boxplot() + stat_summary(fun.y = mean, 
                                color = "black", geom = "point", 
                                shape = 18, size = 3)
```

[ggplot2](#ggplot2) | [factor()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/factor) | [c()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/c) | [aes()](https://www.rdocumentation.org/packages/ggplot2/versions/2.2.1/topics/aes) | [geom_boxplot()](https://www.rdocumentation.org/packages/ggplot2/versions/2.2.1/topics/geom_boxplot) | [stat_summary()](https://www.rdocumentation.org/packages/ggplot2/versions/2.2.1/topics/stat_summary_bin)

***

\newpage

# 3 Basic Data Wrangling and Manipulation

## 3.1 Create a new variable in a data set as a function of existing variables in the data set.

```{r newvar}
# Notice here how you can create the BMI column in the data set just by 
# naming it 
student$BMI <- student$Weight / (student$Height)**2 * 703
head(student, n=5)
```

## 3.2 Create a new variable in a data set using if/else logic of existing variables in the data set.

```{r newvarlogic}
# Notice the use of the ifelse() function for a single condition
student$BMI_Class <- ifelse(student$BMI<19.0, "Underweight", "Healthy")
head(student, n=5)
```

[ifelse()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/ifelse)

## 3.3 Create a new variable in a data set using mathemtical functions applied to existing variables in the data set.

Using the [log()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/log), [exp()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/log), [sqrt()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/MathFun), [ifelse()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/ifelse) and [abs()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/MathFun) functions.
```{r log}
student$LogWeight <- log(student$Weight)
student$ExpAge <- exp(student$Age)
student$SqrtHeight <- sqrt(student$Height)
student$BMI_Neg <- ifelse(student$BMI < 19.0, -student$BMI, student$BMI)
student$BMI_Pos <- abs(student$BMI_Neg)

# Create a Boolean variable
student$BMI_Check <- (student$BMI == student$BMI_Pos)
head(student, n=5)
```

## 3.4 Drop variables from a data set.

```{r drop}
# -c() function tells R not to select the columns listed
student <- subset(student, select = -c(LogWeight, ExpAge, SqrtHeight, 
                                       BMI_Neg, BMI_Pos, BMI_Check))
head(student, n=5)
```

## 3.5 Sort a data set by a variable.

### a) Sort data set by a continuous variable.
```{r sortcont}
student <- student[order(student$Age), ]
# Notice that R uses a stable sorting algorithm by default
head(student, n=5)
```

### b) Sort data set by a categorical variable.
```{r sortcat}
student <- student[order(student$Sex), ]
# Notice that the data is now sorted first by Sex and then within Sex by Age 
head(student, n=5)
```

[order()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/order)

## 3.6 Compute descriptive statistics of continuous variables, grouped by a categorical variable.

```{r descstats}
# Notice the syntax of Age, Height, Weight, and BMI as a function of Sex 
aggregate(cbind(Age, Height, Weight, BMI) ~ Sex, student, mean)
```

[aggregate()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/aggregate) | [cbind()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/cbind)

## 3.7 Add a new row to the bottom of a data set.

```{r newrow}
# Look at the tail of the data currently
tail(student, n=5)

# rbind.data.frame() function binds two data frames together by rows 
student <- rbind.data.frame(student, data.frame(Name='Jane', Sex = 'F', 
                                                Age = 14, Height = 56.3, 
                                                Weight = 77.0, 
                                                BMI = 17.077695, 
                                                BMI_Class = 'Underweight'))
tail(student, n=5)
```

[data.frame()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/data.frame) | [rbind.data.frame()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/cbind)

## 3.8 Create a user-defined function and apply it to a variable in the data set to create a new variable in the data set.

```{r userfunc}
toKG <- function(lb) {
  return(0.45359237 * lb)
}

student$Weight_KG <- toKG(student$Weight)
head(student, n=5)
```

[user-defined functions](http://www.statmethods.net/management/userfunctions.html)

***


# 4 More Advanced Data Wrangling

## 4.1 Drop observations with missing information.

```{r dropmissing}
# Notice the use of the fish data set because it has some missing 
# observations 
fish <- read.csv('/Users/fish.csv')

# First sort by Weight, requesting those with NA for Weight first 
fish <- fish[order(fish$Weight, na.last=FALSE), ]
head(fish, n=5)
```

--

```{r}
new_fish <- na.omit(fish)
head(new_fish, n=5)
```

[na.omit()](https://www.rdocumentation.org/packages/data.table/versions/1.10.4/topics/na.omit.data.table)

## 4.2 Merge two data sets together on a common variable.

### a) First, select specific columns of a data set to create two smaller data sets.

```{r selectvar}
# Notice the use of the student data set again, however we want to reload 
# it without the changes we've made previously  
student <- read.csv('/Users/class.csv')
student1 <- subset(student, select=c(Name, Sex, Age))
head(student1, n=5)
```

--

```{r }
student2 <- subset(student, select=c(Name, Height, Weight))
head(student2, n=5)
```

### b) Second, we want to merge the two smaller data sets on the common variable.

```{r mergevar}
new <- merge(student1, student2)
head(new, n=5)
```

[merge()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/merge)

### c) Finally, we want to check to see if the merged data set is the same as the original data set.

```{r checkvar}
all.equal(student, new)
```

[all.equal()](https://www.rdocumentation.org/packages/data.table/versions/1.10.4/topics/all.equal)

## 4.3 Merge two data sets together by index number only.

### a) First, select specific columns of a data set to create two smaller data sets.

```{r smaller}
newstudent1 <- subset(student, select=c(Name, Sex, Age))
head(newstudent1, n=5)
```

--

```{r}
newstudent2 <- subset(student, select=c(Height, Weight))
head(newstudent2, n=5)
```

### b) Second, we want to join the two smaller data sets.

```{r join}
new2 <- cbind(newstudent1, newstudent2)
head(new2, n=5)
```

### c) Finally, we want to check to see if the joined data set is the same as the original data set.

```{r verify}
all.equal(student, new2)
```

## 4.4 Create a pivot table to summarize information about a data set.

```{r, message= FALSE, warning=FALSE}
# Notice we are using a new data set that needs to be read into the 
# environment
price <- read.csv('/Users/price.csv')

# call the dplyr package
library(dplyr)

# The following code is used to remove the "," and "$" characters from the 
# ACTUAL column so that values can be summed 
price$ACTUAL <- gsub('[$]', '', price$ACTUAL)
price$ACTUAL <- as.numeric(gsub(',', '', price$ACTUAL))

filtered = group_by(price, COUNTRY, STATE, PRODTYPE, PRODUCT)
basic_sum = summarise(filtered, REVENUE = sum(ACTUAL))
head(basic_sum, n=5)
```

[dplyr](#DPLYR) | [group_by](https://www.rdocumentation.org/packages/dplyr/versions/0.5.0/topics/group_by) | [summarise()](https://www.rdocumentation.org/packages/dplyr/versions/0.5.0/topics/summarise)

## 4.5 Return all unique values from a text variable.
```{r pivot, message= FALSE, warning=FALSE}
print(unique(price$STATE))
```
[unique()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/unique)

***

In the following sections, several data set will be used more than once for prediction and modeling. Often, they will be re-read into the environment so we are always going back to the original, raw data.

# 5 Preparation & Basic Regression

## 5.1 Pre-process a data set using principal component analysis.

```{r}
# Notice we are using a new data set that needs to be read into the 
# environment
iris <- read.csv('/Users/iris.csv')
features <- subset(iris, select = -c(Target))

pca <- prcomp(x = features, scale = TRUE)
print(pca)
```

[prcomp()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/prcomp.html)

## 5.2 Split data into training and testing data and export as a .csv file.

```{r, eval = FALSE}
# Set the sample size of the training data
smp_size <- floor(0.7 * nrow(iris))

# set.seed() is used to specify a seed for a random integer so that the 
# results are reproducible
set.seed(29)
train_ind <- sample(seq_len(nrow(iris)), size = smp_size)

train <- iris[train_ind, ]
test <- iris[-train_ind, ]

write.csv(train, file = "/Users/iris_train_R.csv")
write.csv(test, file = "/Users/iris_test_R.csv")
```
[floor()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/Round) | [nrow()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/nrow) | [set.seed()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/Random) | [sample()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/sample) | [seq_len()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/seq) | [write.csv()](https://www.rdocumentation.org/packages/utils/versions/3.4.0/topics/write.table)

## 5.3 Fit a logistic regression model.

```{r logistic}
# Notice we are using a new data set that needs to be read into the 
# environment
tips <- read.csv('/Users/tips.csv')

# The following code is used to determine if the individual left more 
# than a 15% tip 
tips$fifteen <- 0.15 * tips$total_bill
tips$greater15 <- ifelse(tips$tip > tips$fifteen, 1, 0)

# Notice the syntax of greater15 as a function of total_bill 
# You could fit the model of greater15 as a function of all
# other variables with "greater15 ~ ."
logreg <- glm(greater15 ~ total_bill, data = tips, 
              family = "binomial"(link='logit'))
summary(logreg)
```
[glm()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/glm)

## 5.4 Fit a linear regression model.

```{r linear}
# Notice the syntax of tip as function of total_bill
linreg <- lm(tip ~ total_bill, data = tips)
summary(linreg)
```

[lm()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/lm)

***

# 6 Supervised Machine Learning

Many of the following models will make use of the [predict()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/predict) function.

## 6.1 Fit a logistic regression model on training data and assess against testing data.

### a) Fit a logistic regression model on training data.
```{r}
# Notice we are using new data sets that need to be read into the environment
train <- read.csv('/Users/tips_train.csv')
test <- read.csv('/Users/tips_test.csv')

train$fifteen <- 0.15 * train$total_bill
train$greater15 <- ifelse(train$tip > train$fifteen, 1, 0)
test$fifteen <- 0.15 * test$total_bill
test$greater15 <- ifelse(test$tip > test$fifteen, 1, 0)

logreg <- glm(greater15 ~ total_bill, data = train, 
              family = "binomial"(link='logit'))
summary(logreg)
```

### b) Assess the model against the testing data.
```{r}
# Prediction on testing data
predictions <- predict(logreg, test, type = 'response')
predY <- ifelse(predictions < 0.5, 0, 1)

# If the prediction probability is less than 0.5, classify this as a 0
# and otherwise classify as a 1.  This isn't the best method -- a better 
# method would be randomly assigning a 0 or 1 when a probability of 0.5 
# occurrs, but this insures that results are consistent 

# Determine how many were correctly classified
Results <- ifelse(predY == test$greater15, "Correct", "Wrong")
table(Results)
```

[glm()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/glm)

## 6.2 Fit a linear regression model on training data and assess against testing data.

### a) Fit a linear regression model on training data.
```{r}
# Notice we are using new data sets that need to be read into the environment
train <- read.csv('/Users/boston_train.csv')
test <- read.csv('/Users/boston_test.csv')

# Fit a linear regression model
# The "." character tells the model to use all variables except the response 
# variabe (Target)
linreg <- lm(Target ~ ., data = train)
summary(linreg)
```

### b) Assess the model against the testing data.
```{r }
# Predict on testing data
prediction = data.frame(matrix(ncol = 0, nrow = nrow(test)))
prediction$predY = predict(linreg, newdata = test)

# Compute the squared difference between predicted tip and actual tip 
prediction$sq_diff <- (prediction$predY - test$Target)**2

# Compute the mean of the squared differences (mean squared error) 
# as an assessment of the model 
mean_sq_error <- mean(prediction$sq_diff)
print(mean_sq_error)
```

[lm()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/lm)

## 6.3 Fit a decision tree model on training data and assess against testing data.

### a) Fit a decision tree classification model.

#### i) Fit a decision tree classification model on training data and determine variable importance.
```{r, message = FALSE, warning = FALSE}
# Notice we are using new data sets that need to be read into the environment
train <- read.csv('/Users/breastcancer_train.csv')
test <- read.csv('/Users/breastcancer_test.csv')

# call the tree package
library(tree)

treeMod <- tree(Target ~ ., data = train, method = "class")

# Plot the decision tree
plot(treeMod)
text(treeMod)

# Determine variable importance
summary(treeMod)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing data
out <- predict(treeMod, test)
out <- unname(out)
predY <- ifelse(out < 0.5, 0, 1) 

# Determine how many were correctly classified
Results <- ifelse(test$Target == predY, "Correct", "Wrong")
table(Results)
```

[tree](#TREE)

### b) Fit a decision tree regression model.

#### i) Fit a decision tree regression model on training data and determine variable importance.
```{r, message = FALSE, warning = FALSE}
train <- read.csv('/Users/boston_train.csv')
test <- read.csv('/Users/boston_test.csv')

treeMod <- tree(Target ~ ., data = train)

# Plot the decision tree
plot(treeMod)
text(treeMod)

# Determine variable importance
summary(treeMod)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing data
prediction = data.frame(matrix(ncol = 0, nrow = nrow(test)))
prediction$predY = predict(treeMod, newdata = test)

# Determine mean squared error
prediction$sq_diff <- (prediction$predY - test$Target)**2
mean_sq_error <- mean(prediction$sq_diff)
print(mean_sq_error)
```

[tree](#TREE)

## 6.4 Fit a random forest model on training data and assess against testing data.

### a) Fit a random forest classification model.

#### i) Fit a random forest classification model on training data and determine variable importance.
```{r, message = FALSE, warning = FALSE}
train <- read.csv('/Users/breastcancer_train.csv')
test <- read.csv('/Users/breastcancer_test.csv')

# call the randomForest package
library(randomForest)
set.seed(29)

# as.factor() since classification model
rfMod <- randomForest(as.factor(Target) ~ ., data = train)

# Determine variable importance
var_import <- importance(rfMod)
var_import <- data.frame(sort(var_import, decreasing = TRUE, 
                              index.return = TRUE))
var_import$MeanDecreaseGini <- var_import$x 
var_import$X <- var_import$ix - 1
var_import <- subset(var_import, select = -c(ix, x))
head(var_import, n=5)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing data
predY <- predict(rfMod, test)

# Determine how many were correctly classified
Results <- ifelse(test$Target == predY, "Correct", "Wrong")
table(Results)
```

[randomForest](#RANDOM) | [as.factor()](https://stat.ethz.ch/R-manual/R-devel/library/base/html/factor.html)

### b) Fit a random forest regression model.

#### i) Fit a random forest regression model on training data and determine variable importance.
```{r, message = FALSE, warning = FALSE}
train <- read.csv('/Users/boston_train.csv')
test <- read.csv('/Users/boston_test.csv')

# call the randomForest package
library(randomForest)
set.seed(29)

rfMod <- randomForest(Target ~ ., data = train)

# Determine variable importance
var_import <- importance(rfMod)
var_import <- data.frame(sort(var_import, decreasing = TRUE, 
                              index.return = TRUE))
var_import$MeanDecreaseGini <- var_import$x 
var_import$X <- var_import$ix - 1
var_import <- subset(var_import, select = -c(ix, x))
head(var_import, n=5)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing data
prediction = data.frame(matrix(ncol = 0, nrow = nrow(test)))
prediction$predY = predict(rfMod, newdata = test)

# Determine mean squared error
prediction$sq_diff <- (prediction$predY - test$Target)**2
mean_sq_error <- mean(prediction$sq_diff)
print(mean_sq_error)
```

[randomForest](#RANDOM)

## 6.5 Fit a gradient boosting model on training data and assess against testing data.

### a) Fit a gradient boosting classification model.

#### i) Fit a gradient boosting classification model on training data and determine variable importance.
```{r, message = FALSE, warning = FALSE}
train <- read.csv('/Users/breastcancer_train.csv')
test <- read.csv('/Users/breastcancer_test.csv')

# call the gbm package
library(gbm)
set.seed(29)

# distribution = "bernoulli" is appropriate when there are only 2 
# unique values
# n.trees = total number of trees to fit which is analogous to the number 
# of iterations
# shrinkage = learning rate or step-size reduction, whereas a lower 
# learning rate requires more iterations
gbMod <- gbm(Target ~ ., distribution = "bernoulli", data = train, 
             n.trees = 2500, shrinkage = .01)

# Determine variable importance
var_import <- summary(gbMod)
head(var_import, n=5)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing data
out <- predict(object = gbMod, newdata = test, 
                         type = "response", n.trees = 2500)
predY <- ifelse(out < 0.5, 0, 1)

# Determine how many were correctly classified
Results <- ifelse(test$Target == predY, "Correct", "Wrong")
table(Results)
```

[gbm](#GBM)

### b) Fit a gradient boosting regression model.

#### i) Fit a gradient boosting regression model on training data and determine variable importance.
```{r, message = FALSE, warning = FALSE}
train <- read.csv('/Users/boston_train.csv')
test <- read.csv('/Users/boston_test.csv')

# call the gbm package
library(gbm)
set.seed(29)

gbMod <- gbm(Target ~ ., data = train, distribution = "gaussian", 
             n.trees = 2500, shrinkage = .01)

# Determine variable importance
var_import <- summary(gbMod)
head(var_import, n=5)
```
#### ii) Assess the model against the testing data.
```{r}
# Predict the Target in the testing data, remembeing to multiply by 50
prediction = data.frame(matrix(ncol = 0, nrow = nrow(test)))
prediction$predY <- predict(object = gbMod, newdata = test, 
                            type = "response", n.trees = 2500)

# Compute mean squared error 
prediction$sq_diff <- (prediction$predY - test$Target)**2
mean_sq_error <- mean(prediction$sq_diff)
print(mean_sq_error)
```

[gbm](#GBM)

## 6.6 Fit an extreme gradient boosting model on training data and assess against testing data.

### a) Fit an extreme gradient boosting classification model.

#### i) Fit an extreme gradient boosting classification model on training data.
```{r, message = FALSE, warning = FALSE}
train <- read.csv('/Users/breastcancer_train.csv')
test <- read.csv('/Users/breastcancer_test.csv')

# call the xgboost package
library(xgboost)
set.seed(29)

# Fit the model
xgbMod <- xgboost(data.matrix(subset(train, select = -c(Target))), 
                 data.matrix(train$Target), max_depth = 3, nrounds = 2,
                 objective = "binary:logistic", n_estimators = 2500, 
                 shrinkage = .01)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing data
predictions <- predict(xgbMod, 
                       data.matrix(subset(test, 
                                          select = -c(Target))))
predY <- ifelse(predictions < 0.5, 0, 1)

# Determine how many were correctly classified
Results <- ifelse(test$Target == predY, "Correct", "Wrong")
table(Results)
```

[xgboost](#XGBOOST)

### b) Fit an extreme gradient boosting regression model.

#### i) Fit an extreme gradient boosting regression model on training data.
```{r, message = FALSE, warning = FALSE}
train <- read.csv('/Users/boston_train.csv')
test <- read.csv('/Users/boston_test.csv')

# call the xgboost package
library(xgboost)
set.seed(29)

# Fit the model
xgbMod <- xgboost(data.matrix(subset(train, select = -c(Target))), 
                 data.matrix(train$Target), max_depth = 3, nrounds = 10,
                 n_estimators = 2500, shrinkage = .01)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing
prediction = data.frame(matrix(ncol = 0, nrow = nrow(test)))
prediction$predY <- predict(xgbMod, 
                    data.matrix(subset(test, select = -c(Target))))

# Compute the squared difference between predicted tip and actual tip 
prediction$sq_diff <- (prediction$predY - test$Target)**2

# Compute the mean of the squared differences (mean squared error) 
# as an assessment of the model 
mean_sq_error <- mean(prediction$sq_diff)
print(mean_sq_error)
```

[xgboost](#XGBOOST)

## 6.7 Fit a support vector model on training data and assess against testing data.

### a) Fit a support vector classification model.

#### i) Fit a support vector classification model on training data.

Note: In implementation scaling should be used.
```{r, warning = FALSE, message = FALSE}
train <- read.csv('/Users/breastcancer_train.csv')
test <- read.csv('/Users/breastcancer_test.csv')

# call the e1071 package
library(e1071)

# Fit a support vector classification model 
svMod <- svm(Target ~ ., train, type = 'C-classification', kernel = 'linear', scale = FALSE)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing data
predY <- unname(predict(svMod, subset(test, select = -c(Target))))

# Determine how many were correctly classified
Results <- ifelse(test$Target == predY, "Correct", "Wrong")
table(Results)
```

[e1071](#e1071) | [svm()](https://www.rdocumentation.org/packages/e1071/versions/1.6-8/topics/svm)

### b) Fit a support vector regression model.

#### i) Fit a support vector regression model on training data.

Note: In implementation scaling should be used.
```{r, warning = FALSE, message = FALSE}
train <- read.csv('/Users/boston_train.csv')
test <- read.csv('/Users/boston_test.csv')

# call the e1071 package
library(e1071)

svMod <- svm(Target ~ ., train, scale = FALSE)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing data
prediction = data.frame(matrix(ncol = 0, nrow = nrow(test)))
prediction$predY <- unname(predict(svMod, test))
prediction$sq_diff <- (prediction$predY - test$Target)**2
print(mean(prediction$sq_diff))
```

[e1071](#e1071) | [svm()](https://www.rdocumentation.org/packages/e1071/versions/1.6-8/topics/svm)

## 6.8 Fit a neural network model on training data and assess against testing data.

### a) Fit a neural network classification model.

#### i) Fit a neural network classification model on training data.
```{r, warning = FALSE, message = FALSE}
# Notice we are using new data sets
train <- read.csv('/Users/digits_train.csv')
test <- read.csv('/Users/digits_test.csv')

trainInputs <- subset(train, select = -c(Target))
testInputs <- subset(test, select = -c(Target))

# call the RSNNS package
library(RSNNS)
set.seed(29)

trainTarget <- decodeClassLabels(train$Target)
testTarget <- decodeClassLabels(test$Target)

# Fit neural network regression model
nnMod <- mlp(trainInputs, trainTarget, size = c(100), maxit = 200)
```

#### ii) Assess the model against the testing data.
```{r}
# Prediction on testing data 
predictions <- predict(nnMod, testInputs)

# Determine how many were correctly classified
confusionMatrix(testTarget, predictions)
```

[RSNNS](#RSNNS) | [confusionMatrix()](https://www.rdocumentation.org/packages/RSNNS/versions/0.4-9/topics/confusionMatrix)

### b) Fit a neural network regression model.

#### i) Fit a neural network regression model on training data.
```{r, warning = FALSE, message = FALSE}
train <- read.csv('/Users/boston_train.csv')
test <- read.csv('/Users/boston_test.csv')

# call the RSNNS package
library(RSNNS)
set.seed(29)

# Scale input data
scaled_train <- data.frame(scale(subset(train, select = -c(Target))))
scaled_test <- data.frame(scale(subset(test, select = -c(Target))))

# Fit neural network regression model, dividing target by 50 for scaling
nnMod <- mlp(scaled_train, train$Target / 50, maxit = 250, size = c(100))
```

[scale()](https://www.rdocumentation.org/packages/raster/versions/2.5-8/topics/scale)

```{r}
# Assess against testing data, remembering to multiply by 50
preds = data.frame(matrix(ncol = 0, nrow = nrow(test)))
preds$predY <- predict(nnMod, scaled_test)*50
preds$sq_error <- (preds$predY - test$Target)**2
print(mean(preds$sq_error))
```

[RSNNS](#RSNNS)

***

# 7 Unsupervised Machine Learning

## 7.1 KMeans Clustering

```{r}
iris = read.csv('/Users/iris.csv')

iris$Species = ifelse(iris$Target == 0, "Setosa", 
                      ifelse(iris$Target == 1, "Versicolor", "Virginica"))

features <- as.matrix(subset(iris, select = c(PetalLength, PetalWidth, 
                                              SepalLength, SepalWidth)))

set.seed(29)

kmeans <- kmeans(features, 3)

table(iris$Species, kmeans$cluster)
```

[kmeans()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html)

## 7.2 Spectral Clustering

```{r, warning = FALSE, message = FALSE}
# call the kernlab package
library(kernlab)

set.seed(29)

spectral <- specc(features, centers = 3, iterations = 10, nystrom.red = TRUE)

labels <- as.data.frame(spectral)

table(iris$Species, labels$spectral)
```

[kernlab](#KERNLAB) | [specc()](https://artax.karlin.mff.cuni.cz/r-help/library/kernlab/html/specc.html)

## 7.3 Ward Hierarchical Clustering

```{r}
set.seed(29)

hclust <- hclust(dist(features), method = "ward.D2")

table(iris$Species, cutree(hclust, 3))
```

[Hierarchical Clustering in R](https://www.r-bloggers.com/hierarchical-clustering-in-r-2/) | [hclust()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html)

## 7.4 DBSCAN

```{r, warning = FALSE, message = FALSE}
# call the dbscan package
library(dbscan)

set.seed(29)

# eps = 0.5 is default in Python
dbscan <- dbscan(features, eps = 0.5)

table(iris$Species, dbscan$cluster)
```

[dbscan](#DBSCAN)

## 7.5 Self-organized map

```{r, warning = FALSE, message = FALSE}
# call the kohonen package
library(kohonen)

# Seed chosen to match SAS and R results
set.seed(5)

fit <- som(features, mode = "online", somgrid(4, 4, "rectangular"))

plot(fit, type = "dist.neighbour", shape = "straight")
```

[kohonen](#KOHONEN)

***

# 8 Forecasting

## 8.1 Fit an ARIMA model to a timeseries.

### a) Plot the timeseries.
```{r}
# Read in new data set
air <- read.csv('/Users/air.csv')

air_series <- air$AIR

plot.ts(air_series, ylab="Air")
```

[plot.ts()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/plot.ts.html)

### b) Fit an ARIMA (0, 1, 1) model and predict 2 years (24 months).
```{r, warning = FALSE, message = FALSE}
a_fit <- arima(air_series, order = c(0,1,1), 
               seasonal = list(order = c(0,1,1), period = 12),
               method = "ML")

# call the forecast package
library(forecast)

a_forecast <- forecast(a_fit, 24)

plot(a_forecast, xlab = "Month", ylab = "Air")
```

[arima()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html) | [forecast](#FORECAST)

## 8.2 Fit a Simple Exponential Smoothing model to a timeseries.

### a) Plot the timeseries.
```{r}
# Read in new data set
usecon <- read.csv('/Users/usecon.csv')

petrol_series <- usecon$PETROL

petrol <- ts(petrol_series, frequency = 12)

plot.ts(petrol, ylab="Petrol")
```

[ts()](http://www.statmethods.net/advstats/timeseries.html) | [plot.ts()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/plot.ts.html)

### b) Fit a Simple Exponential Smoothing model, predict 2 years (24 months) out and plot predictions.
```{r, message = FALSE, warning = FALSE}
# call the forecast package
library(forecast)

ses_fit <- ses(petrol, h=24, alpha = 0.9999)

plot(ses_fit, xlab = "Month", ylab = "Petrol")
```

[forecast](#FORECAST)

## 8.3 Fit a Holt-Winters model to a timeseries.

### a) Plot the timeseries.
```{r}
vehicle_series <- usecon$VEHICLES

vehicle <- ts(vehicle_series, frequency = 12)

plot.ts(vehicle, ylab="Vehicle")
```

[ts()](http://www.statmethods.net/advstats/timeseries.html) | [plot.ts()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/plot.ts.html)

### b) Fit a Holt-Winters additive model, predict 2 years (24 months) out and plot predictions.
```{r, warning = FALSE, message = FALSE}
# call the forecast package
library(forecast)

add_fit <- HoltWinters(vehicle, seasonal = "additive")

add_forecast <- forecast(add_fit, 24)

plot(add_forecast)
```

[forecast](#FORECAST)

## 8.4 Fit a Facebook Prophet forecasting model to a timeseries.
```{r, warning=FALSE, message=FALSE}
air <- read.csv('/Users/air.csv')

# call the prophet & dplyr packages
library(prophet)
library(dplyr)

air_df <- data.frame(matrix(ncol = 0, nrow = nrow(air)))

air_df$ds <- as.Date(air$DATE, format = "%m/%d/%Y")
air_df$y <- air$AIR

m <- prophet(air_df, yearly.seasonality = TRUE, weekly.seasonality = FALSE)

future <- make_future_dataframe(m, periods = 24, freq = "month")

forecast <- predict(m, future)

plot(m, forecast)
```

[Facebook Prophet R API](#PROPHET)

***

# 9 Model Evaluation & Selection

## 9.1 Evaluate the accuracy of regression models.

### a) Evaluation on training data.
```{r, warning = FALSE, message = FALSE}
train <- read.csv('/Users/boston_train.csv')
test <- read.csv('/Users/boston_test.csv')

set.seed(29)

# Random Forest Regression Model
# call the randomForest package
library(randomForest)

rfMod <- randomForest(Target ~ ., data = train)

# Evaluation on training data
predY <- predict(rfMod, train)
predY <- unname(predY)

# Determine coefficient of determination score
r2_rf <- 1 - ( (sum((train$Target - 
                       predY)**2)) / (sum((train$Target - 
                                           mean(train$Target))**2)) )
print(paste0("Random forest regression model r^2 score (coefficient of determination): ", r2_rf))
```

### b) Evaluation on testing data.
```{r}
# Random Forest Regression Model (rfMod) 

# Evaluation on testing data
predY <- predict(rfMod, test)
predY <- unname(predY)

# Determine coefficient of determination score
r2_rf = 1 - ( (sum((test$Target - 
                      predY)**2)) / (sum((test$Target - 
                                          mean(test$Target))**2)) )
print(paste0("Random forest regression model r^2 score (coefficient of determination): ", r2_rf))
```

[randomForest](#RANDOM) | [predict()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/predict) | [unname()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/unname)

The formula used here for the coefficient of determination score is based off the Python skearn formula for [r2_score](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination).  For more information about model assessment in R, please review information about the R package [caret](https://cran.r-project.org/web/packages/caret/index.html).

## 9.2 Evaluate the accuracy of classification models.

### a) Evaluation on training data.
```{r, message = FALSE, warning = FALSE}
train <- read.csv('/Users/digits_train.csv')
test <- read.csv('/Users/digits_test.csv')

set.seed(29)

# Random Forest Classification Model
# call the randomForest package
library(randomForest)

rfMod <- randomForest(as.factor(Target) ~ ., data = train)

# Evaluation on training data
predY <- predict(rfMod, train)
predY <- unname(predY)

# Determine accuracy score
accuracy_rf <- (1/nrow(train)) * sum(as.numeric(predY == train$Target))
print(paste0("Random forest model accuracy: ", accuracy_rf))
```

### b) Evaluation on testing data.

```{r, message = FALSE, warning = FALSE}
# Random Forest Classification Model (rfMod)

# Evaluation on testing data
predY <- predict(rfMod, test)
predY <- unname(predY)

# Determine accuracy score
accuracy_rf <- (1/nrow(test)) * sum(as.numeric(predY == test$Target))
print(paste0("Random forest model accuracy: ", accuracy_rf))
```

[randomForest](#RANDOM) | [predict()](https://www.rdocumentation.org/packages/stats/versions/3.4.0/topics/predict) | [unname()](https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/unname)

The formula used here for the accuracy score is based off the Python skearn formula for [accuracy_score](http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score).  For more information about model assessment in R, please review information about the R package [caret](https://cran.r-project.org/web/packages/caret/index.html).

## 9.3 Evaluation with cross validation.

### a) KFold
```{r, warning = FALSE, message = FALSE}
# Notice we are using a new data set that needs to be read into the 
# environment 
breastcancer = read.csv('/Users/breastcancer.csv')

# call the caret and randomForest packages
library(caret)
library(randomForest)

set.seed(29)

# Create the 5 cross validation folds
train_control <- trainControl(method = "cv", number = 5, 
                              savePredictions = TRUE)

# Convert Target into a factor variable for the random forest model
breastcancer$Target <- factor(breastcancer$Target, levels = c(1,0), 
                              labels = c(1, 0))

# Train the model, using the 5 cross validation folds
model <- train(Target~., data = breastcancer, trControl = train_control, 
               method = "rf")

# Assess the accuracy of the model
tab <- model$pred
tab$correct <- (tab$pred == tab$obs)
tab$correct_num <- ifelse(tab$correct=="TRUE", 1, 0)
aggdata <- unname(as.matrix(aggregate(correct_num ~ Resample, tab, sum)))
aggdata <- as.numeric(aggdata[,2])
counts <- unname(table(tab$Resample))
accuracy <- c(0,0,0,0,0)
for (i in 1:5) {
  accuracy[i] <- aggdata[i]/counts[i]
}

print(paste0("Accuracy: ", round(mean(accuracy)*100, digits=2), "% +/- ", 
             round(sd(accuracy)*100, digits=2), "%"))
```

[caret](#CARET) | [randomForest](#RANDOM) | [trainControl()](https://www.rdocumentation.org/packages/caret/versions/6.0-76/topics/trainControl)

### b) ShuffleSplit
```{r, warning = FALSE, message = FALSE}
# call the caret and randomForest packages
library(caret)
library(randomForest)

set.seed(29)

X = subset(breastcancer, select = -c(Target))
Y = breastcancer$Target

# Create the data partition
trainIndex <- createDataPartition(Y, times = 5, p = 0.7, list = FALSE)
accuracy <- c(0, 0, 0, 0, 0)

for (i in 1:5) {
  nam <- paste("data_train", i, sep ="")
  assign(nam, breastcancer[trainIndex[,i],])
  nam <- paste("data_test", i, sep ="")
  assign(nam, breastcancer[-trainIndex[,i],])
}

data_train <- list(data_train1, data_train2, data_train3, data_train4, 
                   data_train5)
data_test <- list(data_test1, data_test2, data_test3, data_test4, data_test5)

# Train the model and assess the accuracy
for (i in 1:5) {
  fit <- randomForest(as.factor(Target) ~ ., data = data_train[[i]])
  Prediction <- predict(fit, data_test[[i]])
  Prediction <- unname(Prediction)
  correct <- (data_test[[i]]$Target == Prediction)
  counts <- unname(table(correct))
  accuracy[i] <- counts[2] / sum(counts)
}

print(paste0("Accuracy: ", round(mean(accuracy)*100, digits=2), "% +/- ", 
             round(sd(accuracy)*100, digits=2), "%"))
```

[caret](#CARET) | [randomForest](#RANDOM) | [createDataPartition()](https://www.rdocumentation.org/packages/caret/versions/6.0-76/topics/createDataPartition) 

***

# Appendix

## 1 Built-in R Objects

#### [Vectors](#vectors)
* [Logical](https://stat.ethz.ch/R-manual/R-devel/library/base/html/logical.html)
* [Numeric](https://stat.ethz.ch/R-manual/R-devel/library/base/html/numeric.html)
* [Integer](https://stat.ethz.ch/R-manual/R-devel/library/base/html/integer.html)
* [Complex](https://stat.ethz.ch/R-manual/R-devel/library/base/html/complex.html)
* [Character](https://stat.ethz.ch/R-manual/R-devel/library/base/html/character.html)
* [Raw](https://stat.ethz.ch/R-manual/R-devel/library/base/html/raw.html)

#### [Lists](#list)

#### [Matrics](https://stat.ethz.ch/R-manual/R-devel/library/base/html/matrix.html)

#### [Arrays](#array)

#### [Factors](https://stat.ethz.ch/R-manual/R-devel/library/base/html/factor.html)

#### [Data Frames](#DataFrame)

## 2 R packages used in this tutorial

#### [gdata](https://cran.r-project.org/web/packages/gdata/gdata.pdf) {#GDATA}
Data manipulation

#### [rjson](https://cran.r-project.org/web/packages/rjson/rjson.pdf) {#RJSON}
Converting R objects into JSON objects, and JSON objects into R objects

#### [ggplot2](https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf) {#ggplot2}
Visualizations and graphics

#### [dplyr](https://cran.r-project.org/web/packages/dplyr/dplyr.pdf) {#DPLYR}
Working with [data frame](#DataFrame) like objects

#### [tree](https://cran.r-project.org/web/packages/tree/tree.pdf) {#TREE}
Decision trees models

#### [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) {#RANDOM}
Random forest models

#### [gbm](https://cran.r-project.org/web/packages/gbm/gbm.pdf) {#GBM}
Gradient boosting models

#### [xgboost](https://cran.r-project.org/web/packages/xgboost/xgboost.pdf) {#XGBOOST}
Extreme gradient boosting models

#### [e1071](https://cran.r-project.org/web/packages/e1071/e1071.pdf) {#e1071}
Support vector machine models

#### [RSNNS](https://cran.r-project.org/web/packages/RSNNS/RSNNS.pdf) {#RSNNS}
Neural network models

#### [caret](https://cran.r-project.org/web/packages/caret/caret.pdf) {#CARET}
Training and plotting classification and regression models

#### [kernlab](https://cran.r-project.org/web/packages/kernlab/kernlab.pdf) {#KERNLAB}
Spectral clustering

#### [dbscan](https://cran.r-project.org/web/packages/dbscan/dbscan.pdf) {#DBSCAN}
DBSCAN clustering

#### [kohonen](https://cran.r-project.org/web/packages/kohonen/kohonen.pdf) {#KOHONEN}
Supervised and unsupervised self-organizing maps

#### [forecast](https://cran.r-project.org/web/packages/forecast/forecast.pdf) {#FORECAST}
Displaying and analyzing time series for forecasting

#### [prophet](https://facebookincubator.github.io/prophet/docs/quick_start.html#r-api) {#PROPHET}
Tools for forecasting using the Facebook Prophet model

***

# Alphabetical Index

## [Array](https://www.tutorialspoint.com/r/r_arrays.htm) {#array}
A one-dimensional data frame.  Please see the following example of array creation and access:
```{r}
my_array <- c(1, 3, 5, 9)
print(my_array)
print(my_array[1])
```

## [Data Frame](https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html) {#DataFrame}
An R Data Frame is a two-dimensional tabular structure with labeled axes (rows and columns), where data observations are represented by rows and data variables are represented by columns.

## Dictionary
A dictionary is an associative array which is indexed by keys which map to values.  Therefore, a dictionary is an unordered set of key:value pairs where each key is unique.  In R, a dictionary can be implemented using a [named list](http://www.r-tutor.com/r-introduction/list/named-list-members).  Please see the following example of named list creation and access:
```{r}
student <- read.csv('/Users/class.csv')
values <- student$Age
names(values) <- student$Name
print(values["James"])
```

## [List](http://www.r-tutor.com/r-introduction/list) {#list}
An R list is a sequence of comma-separated objects that need not be of the same type.  Please see the following example of list creation and access:

```{r}
list1 <- list('item1', 102)
print(list1)
print(list1[1])
```

## [Vector](https://www.tutorialspoint.com/r/r_data_types.htm) {#vector}
A vector is a one-dimensional data structure which is able to hold different classes of elements, but only one class per vector.

***

For more information on R packages and functions, along with helpful examples, please see [R](https://www.rdocumentation.org/).

